{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2161565",
   "metadata": {},
   "source": [
    "# 1. Importating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f433be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rarity</th>\n",
       "      <th>last_sale_price</th>\n",
       "      <th>sale_count</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.959827</td>\n",
       "      <td>3.88</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.037520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244.148047</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.457356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187.401798</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.598466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184.405342</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.725284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>186.405769</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.909261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rarity  last_sale_price  sale_count  predicted_price\n",
       "0   56.959827             3.88         6.0         3.037520\n",
       "1  244.148047             2.80         5.0         3.457356\n",
       "2  187.401798             0.40         3.0         2.598466\n",
       "3  184.405342             4.50         1.0         2.725284\n",
       "4  186.405769             2.99         2.0         2.909261"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To remove the scientific notation from numpy arrays\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "NFTdata=pd.read_csv('dataset/train-data.csv')\n",
    "NFTdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff40b3",
   "metadata": {},
   "source": [
    "# 2. Splitting Data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a33bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3487, 3)\n",
      "(3487, 1)\n",
      "(1495, 3)\n",
      "(1495, 1)\n"
     ]
    }
   ],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "Predictors=['rarity', 'last_sale_price', 'sale_count']\n",
    "TargetVariable=['predicted_price']\n",
    "\n",
    "X=NFTdata[Predictors].values\n",
    "y=NFTdata[TargetVariable].values\n",
    "\n",
    "### Sandardization of data ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "PredictorScaler=StandardScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    "\n",
    "# Generating the standardized values of X and y\n",
    "X=PredictorScalerFit.transform(X)\n",
    "\n",
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25a0f6",
   "metadata": {},
   "source": [
    "# 3. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cd7aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2fba2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "175/175 [==============================] - 0s 808us/step - loss: 8.8001\n",
      "Epoch 2/40\n",
      "175/175 [==============================] - 0s 839us/step - loss: 3.4142\n",
      "Epoch 3/40\n",
      "175/175 [==============================] - 0s 777us/step - loss: 0.2473\n",
      "Epoch 4/40\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.1032\n",
      "Epoch 5/40\n",
      "175/175 [==============================] - 0s 942us/step - loss: 0.0853\n",
      "Epoch 6/40\n",
      "175/175 [==============================] - 0s 794us/step - loss: 0.0738\n",
      "Epoch 7/40\n",
      "175/175 [==============================] - 0s 799us/step - loss: 0.0648\n",
      "Epoch 8/40\n",
      "175/175 [==============================] - 0s 800us/step - loss: 0.0574\n",
      "Epoch 9/40\n",
      "175/175 [==============================] - 0s 817us/step - loss: 0.0523\n",
      "Epoch 10/40\n",
      "175/175 [==============================] - 0s 857us/step - loss: 0.0485\n",
      "Epoch 11/40\n",
      "175/175 [==============================] - 0s 845us/step - loss: 0.0460\n",
      "Epoch 12/40\n",
      "175/175 [==============================] - 0s 777us/step - loss: 0.0440\n",
      "Epoch 13/40\n",
      "175/175 [==============================] - 0s 817us/step - loss: 0.0436\n",
      "Epoch 14/40\n",
      "175/175 [==============================] - 0s 805us/step - loss: 0.0422\n",
      "Epoch 15/40\n",
      "175/175 [==============================] - 0s 982us/step - loss: 0.0417\n",
      "Epoch 16/40\n",
      "175/175 [==============================] - 0s 834us/step - loss: 0.0419\n",
      "Epoch 17/40\n",
      "175/175 [==============================] - 0s 851us/step - loss: 0.0410\n",
      "Epoch 18/40\n",
      "175/175 [==============================] - 0s 839us/step - loss: 0.0403\n",
      "Epoch 19/40\n",
      "175/175 [==============================] - 0s 811us/step - loss: 0.0400\n",
      "Epoch 20/40\n",
      "175/175 [==============================] - 0s 885us/step - loss: 0.0398\n",
      "Epoch 21/40\n",
      "175/175 [==============================] - 0s 822us/step - loss: 0.0396\n",
      "Epoch 22/40\n",
      "175/175 [==============================] - 0s 834us/step - loss: 0.0388\n",
      "Epoch 23/40\n",
      "175/175 [==============================] - 0s 822us/step - loss: 0.0390\n",
      "Epoch 24/40\n",
      "175/175 [==============================] - 0s 857us/step - loss: 0.0389\n",
      "Epoch 25/40\n",
      "175/175 [==============================] - 0s 857us/step - loss: 0.0381\n",
      "Epoch 26/40\n",
      "175/175 [==============================] - 0s 834us/step - loss: 0.0382\n",
      "Epoch 27/40\n",
      "175/175 [==============================] - 0s 834us/step - loss: 0.0378\n",
      "Epoch 28/40\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 29/40\n",
      "175/175 [==============================] - 0s 862us/step - loss: 0.0371\n",
      "Epoch 30/40\n",
      "175/175 [==============================] - 0s 794us/step - loss: 0.0372\n",
      "Epoch 31/40\n",
      "175/175 [==============================] - 0s 800us/step - loss: 0.0375\n",
      "Epoch 32/40\n",
      "175/175 [==============================] - 0s 851us/step - loss: 0.0365\n",
      "Epoch 33/40\n",
      "175/175 [==============================] - 0s 822us/step - loss: 0.0366\n",
      "Epoch 34/40\n",
      "175/175 [==============================] - 0s 811us/step - loss: 0.0363\n",
      "Epoch 35/40\n",
      "175/175 [==============================] - 0s 800us/step - loss: 0.0364\n",
      "Epoch 36/40\n",
      "175/175 [==============================] - 0s 919us/step - loss: 0.0370\n",
      "Epoch 37/40\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "Epoch 38/40\n",
      "175/175 [==============================] - 0s 931us/step - loss: 0.0359\n",
      "Epoch 39/40\n",
      "175/175 [==============================] - 0s 799us/step - loss: 0.0356\n",
      "Epoch 40/40\n",
      "175/175 [==============================] - 0s 800us/step - loss: 0.0357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2459f0e6dc8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create ANN model\n",
    "model = Sequential()\n",
    "\n",
    "# Defining the Input layer and FIRST hidden layer, both are same!\n",
    "model.add(Dense(units=5, input_dim=len(Predictors), kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# Defining the Second layer of the model\n",
    "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 20, epochs = 40, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d52b895",
   "metadata": {},
   "source": [
    "# 4. Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e6cc0",
   "metadata": {},
   "source": [
    "## 4.1. Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73db9fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is:  96.38 % accurate\n"
     ]
    }
   ],
   "source": [
    "MAPE = np.mean(100 * (np.abs(y_test-model.predict(X_test))/y_test))\n",
    "print(\"The model is: \", \"{:.2f}\".format(100-MAPE), \"% accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ea7aa",
   "metadata": {},
   "source": [
    "## 4.2. Comparison between the actual price and the predicted price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40fbf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rarity</th>\n",
       "      <th>last_sale_price</th>\n",
       "      <th>sale_count</th>\n",
       "      <th>Price</th>\n",
       "      <th>PredictedPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126.486504</td>\n",
       "      <td>4.1000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.096494</td>\n",
       "      <td>3.068604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>257.417478</td>\n",
       "      <td>1.8000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.299036</td>\n",
       "      <td>3.310687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>210.306732</td>\n",
       "      <td>5.8000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.437621</td>\n",
       "      <td>3.273286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214.117371</td>\n",
       "      <td>8.2000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.398477</td>\n",
       "      <td>3.427645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193.813751</td>\n",
       "      <td>6.1666</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.602293</td>\n",
       "      <td>3.730504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rarity  last_sale_price  sale_count     Price  PredictedPrice\n",
       "0  126.486504           4.1000         7.0  3.096494        3.068604\n",
       "1  257.417478           1.8000         1.0  3.299036        3.310687\n",
       "2  210.306732           5.8000         4.0  3.437621        3.273286\n",
       "3  214.117371           8.2000         6.0  3.398477        3.427645\n",
       "4  193.813751           6.1666        13.0  3.602293        3.730504"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating Predictions on testing data\n",
    "Predictions=model.predict(X_test)\n",
    "\n",
    "# # Scaling the test data back to original scale\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    "\n",
    "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
    "TestingData['Price']=y_test\n",
    "TestingData['PredictedPrice']=Predictions\n",
    "TestingData.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
