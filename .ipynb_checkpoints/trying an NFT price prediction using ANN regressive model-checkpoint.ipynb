{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2161565",
   "metadata": {},
   "source": [
    "# 1. importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f433be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rarity</th>\n",
       "      <th>last_sale_price</th>\n",
       "      <th>sale_count</th>\n",
       "      <th>predicted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.959827</td>\n",
       "      <td>3.88</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.037520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244.148047</td>\n",
       "      <td>2.80</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.457356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187.401798</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.598466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184.405342</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.725284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>186.405769</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.909261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rarity  last_sale_price  sale_count  predicted_price\n",
       "0   56.959827             3.88         6.0         3.037520\n",
       "1  244.148047             2.80         5.0         3.457356\n",
       "2  187.401798             0.40         3.0         2.598466\n",
       "3  184.405342             4.50         1.0         2.725284\n",
       "4  186.405769             2.99         2.0         2.909261"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the cleaned numeric car prices data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To remove the scientific notation from numpy arrays\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "NFTdata=pd.read_csv('dataset/train-data.csv')\n",
    "NFTdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0dca8",
   "metadata": {},
   "source": [
    "# 2. Splitting Data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable=['predicted_price']\n",
    "Predictors=['rarity', 'last_sale_price', 'sale_count']\n",
    "\n",
    "X=CarPricesDataNumeric[Predictors].values\n",
    "y=CarPricesDataNumeric[TargetVariable].values\n",
    "\n",
    "### Sandardization of data ###\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "PredictorScaler=StandardScaler()\n",
    "TargetVarScaler=StandardScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    "TargetVarScalerFit=TargetVarScaler.fit(y)\n",
    "\n",
    "# Generating the standardized values of X and y\n",
    "X=PredictorScalerFit.transform(X)\n",
    "y=TargetVarScalerFit.transform(y)\n",
    "\n",
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f9aee",
   "metadata": {},
   "source": [
    "# 3. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d85178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# create ANN model\n",
    "model = Sequential()\n",
    "\n",
    "# Defining the Input layer and FIRST hidden layer, both are same!\n",
    "model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# Defining the Second layer of the model\n",
    "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "model.add(Dense(units=5, kernel_initializer='normal', activation='tanh'))\n",
    "\n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 20, epochs = 50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457ed51",
   "metadata": {},
   "source": [
    "# 4. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the best parameters for ANN\n",
    "def FunctionFindBestParams(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    batch_size_list=[5, 10, 15, 20]\n",
    "    epoch_list  =   [5, 10, 50, 100]\n",
    "    \n",
    "    import pandas as pd\n",
    "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
    "    \n",
    "    # initializing the trials\n",
    "    TrialNumber=0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=5, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "            # Compiling the model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0)\n",
    "\n",
    "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(X_test))/y_test))\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', 100-MAPE)\n",
    "            \n",
    "            SearchResultsData=SearchResultsData.append(pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n",
    "                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] ))\n",
    "    return(SearchResultsData)\n",
    "\n",
    "\n",
    "######################################################\n",
    "# Calling the function\n",
    "ResultsData=FunctionFindBestParams(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34212e3",
   "metadata": {},
   "source": [
    "# 5. Plotting the parameter trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ResultsData.plot(x='Parameters', y='Accuracy', figsize=(15,4), kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c161d40",
   "metadata": {},
   "source": [
    "# 6. Training the ANN model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfc517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 15, epochs = 5, verbose=0)\n",
    "\n",
    "# Generating Predictions on testing data\n",
    "Predictions=model.predict(X_test)\n",
    "\n",
    "# Scaling the predicted Price data back to original price scale\n",
    "Predictions=TargetVarScalerFit.inverse_transform(Predictions)\n",
    "\n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
    "\n",
    "# Scaling the test data back to original scale\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    "\n",
    "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
    "TestingData['Price']=y_test_orig\n",
    "TestingData['PredictedPrice']=Predictions\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114108b3",
   "metadata": {},
   "source": [
    "# 6. Finding the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the absolute percent error\n",
    "APE=100*(abs(TestingData['Price']-TestingData['PredictedPrice'])/TestingData['Price'])\n",
    "TestingData['APE']=APE\n",
    "\n",
    "print('The Accuracy of ANN model is:', 100-np.mean(APE))\n",
    "TestingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3ede1",
   "metadata": {},
   "source": [
    "# 7. Finding best hyperparameters using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate Deep ANN model \n",
    "def make_regression_ann(Optimizer_trial):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Optimizer_trial)\n",
    "    return model\n",
    "\n",
    "###########################################\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Listing all the parameters to try\n",
    "Parameter_Trials={'batch_size':[10,20,30],\n",
    "                      'epochs':[10,20],\n",
    "                    'Optimizer_trial':['adam', 'rmsprop']\n",
    "                 }\n",
    "\n",
    "# Creating the regression ANN model\n",
    "RegModel=KerasRegressor(make_regression_ann, verbose=0)\n",
    "\n",
    "###########################################\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Defining a custom function to calculate accuracy\n",
    "def Accuracy_Score(orig,pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
    "    print('#'*70,'Accuracy:', 100-MAPE)\n",
    "    return(100-MAPE)\n",
    "\n",
    "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
    "\n",
    "#########################################\n",
    "# Creating the Grid search space\n",
    "# See different scoring methods by using sklearn.metrics.SCORERS.keys()\n",
    "grid_search=GridSearchCV(estimator=RegModel, \n",
    "                         param_grid=Parameter_Trials, \n",
    "                         scoring=custom_Scoring, \n",
    "                         cv=5)\n",
    "\n",
    "#########################################\n",
    "# Measuring how much time it took to find the best params\n",
    "import time\n",
    "StartTime=time.time()\n",
    "\n",
    "# Running Grid Search for different paramenters\n",
    "grid_search.fit(X,y, verbose=1)\n",
    "\n",
    "EndTime=time.time()\n",
    "print(\"########## Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes')\n",
    "\n",
    "print('### Printing Best parameters ###')\n",
    "grid_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
